{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "210b0d9d-7687-4853-b593-280caae27568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install emoji --upgrade\n",
    "# ! pip install twarc\n",
    "# ! pip install twarc-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62067fb5-8c1c-46dc-9e9b-1f74c67ac233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e79a2b2-9569-4fa6-abb4-76ded6166e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2677c-34dc-4cae-91a6-da2352883fbf",
   "metadata": {},
   "source": [
    "# Prerequisite functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "08fe033b-3786-4ca5-aaf6-1eb0e45cff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger(log_file_path=\"debug.log\", level=\"INFO\"):\n",
    "# def set_logger(log_file_path=\"debug.log\", level=\"DEBUG\"):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level)\n",
    "    scream_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    logger.addHandler(scream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "    \n",
    "\n",
    "\n",
    "try:\n",
    "    # print(len(logger.handlers))\n",
    "    while len(logger.handlers) > 1:\n",
    "        logger.handlers.pop(0)\n",
    "        # print(len(logger.handlers))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logger = set_logger()\n",
    "\n",
    "\n",
    "\n",
    "def is_too_many_requests(json_response, start_time):\n",
    "    is_too_many = False\n",
    "    now = time.perf_counter()    \n",
    "    elapsed_time = now - start_time\n",
    "    time_window = 15 * 60 # seconds, 15 min\n",
    "    title = json_response.get('title', \"\")\n",
    "    if title =='Too Many Requests':            \n",
    "        is_to_many = True\n",
    "        print(f\"To many requests, will sleep {int(time_window - elapsed_time)} seconds.\")\n",
    "        time.sleep(time_window)\n",
    "        \n",
    "    return is_too_many, elapsed_time\n",
    "\n",
    "\n",
    "#-------------------- merge results -------------------#          \n",
    "def find_place_id(row):\n",
    "    # print(row)\n",
    "    \n",
    "    cell_text = row.get(\"geo\", \"\")\n",
    "    # print(cell_text)\n",
    "    if len(cell_text) > 1:\n",
    "        place_dict = ast.literal_eval(cell_text)\n",
    "    else:\n",
    "        return \"\"\n",
    "    # print(place_dict)\n",
    "    if isinstance(place_dict, dict):\n",
    "        place_id = place_dict.get(\"place_id\", \"\")\n",
    "        if len(place_id) > 1:\n",
    "            return place_id\n",
    "\n",
    "def clean_tweets(row):\n",
    "    \n",
    "    text = row['text'].replace('\\n',' ').replace(\",\", \";\").replace('\\r', '').replace('\\t', ' ').strip()\n",
    "    return text\n",
    "\n",
    "def find_poll_id(row):\n",
    "    \n",
    "    text = row['text'].replace('\\n',' ').replace(\",\", \";\").replace('\\r', '').replace('\\t', ' ').strip()\n",
    "    return text\n",
    "\n",
    "def refine_data(df):\n",
    "    df['place_id'] = df.apply(find_place_id, axis=1)\n",
    "    df['text'] = df.apply(clean_tweets, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def find_media_row(row, df_media):\n",
    "    cell_text = row[\"attachments\"]\n",
    "    if len(cell_text) > 1:\n",
    "        attachments_dict = ast.literal_eval(cell_text)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "    if isinstance(attachments_dict, dict):\n",
    "        media_keys = attachments_dict.get(\"media_keys\", \"\")\n",
    "        media_rows = []\n",
    "        # print(df_media)\n",
    "        # print(attachments_dict)\n",
    "        for key in media_keys:\n",
    "            key = str(key)\n",
    "            if len(key) > 1:\n",
    "                print(key)\n",
    "                print(df_media['media_table_media_key'])\n",
    "                row = df_media[df_media['media_table_media_key']==key].iloc[0]#.to_json(orient='values')[1:-1]\n",
    "                row = json.dumps(row)\n",
    "                print(df_media[df_media['media_table_media_key']==key])\n",
    "                media_rows.append(row)\n",
    "        # print(media_rows)\n",
    "        return media_rows\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def find_poll_row(row, df_poll):\n",
    "    cell_text = row[\"attachments\"]\n",
    "    if len(cell_text) > 1:\n",
    "        attachments_dict = ast.literal_eval(cell_text)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "    if isinstance(attachments_dict, dict):\n",
    "        poll_ids = attachments_dict.get(\"poll_ids\", \"\")\n",
    "        poll_rows = []\n",
    "        # print(df_poll)\n",
    "        # print(attachments_dict)\n",
    "        for i in poll_ids:\n",
    "            i = str(i)\n",
    "            if len(i) > 1:\n",
    "                # print(i)\n",
    "                # print(df_poll['polls_table_id'])\n",
    "                print(\"df_poll['polls_table_id']\", df_poll['polls_table_id'])\n",
    "                row = df_poll[df_poll['polls_table_id']==i].iloc[0]#.to_json(orient='values')[1:-1]\n",
    "                print(\"row in find_poll_row():\", row)\n",
    "                row = json.dumps(row)\n",
    "#                 print(df_poll[df_poll['polls_table_media_id']==i])\n",
    "                poll_rows.append(row)\n",
    "        # print(poll_rows)\n",
    "        return poll_rows\n",
    "    return \"\"\n",
    "\n",
    "def get_lonlat(row):\n",
    "    row[\"lon\"] = \"\"\n",
    "    row[\"lat\"] = \"\"\n",
    "#     print('row[places_table_geo]:', row[\"places_table_geo\"])\n",
    "    if len(row[\"places_table_geo\"]) > 1:\n",
    "        geo_dict = ast.literal_eval(row[\"places_table_geo\"])\n",
    "#         print('geo_dict:', geo_dict)\n",
    "        bbox = geo_dict.get(\"bbox\", [])\n",
    "        if len(bbox) == 4:\n",
    "            row[\"lon\"] = (bbox[0] + bbox[2]) / 2\n",
    "            row[\"lat\"] = (bbox[1] + bbox[3]) / 2\n",
    "    return row\n",
    "\n",
    "def merge_results(saved_path, is_zipped=False):\n",
    "    if is_zipped:\n",
    "        suffix = '.csv.gz'\n",
    "    else:\n",
    "        suffix = '.csv'\n",
    "    \n",
    "    data_files = glob.glob(os.path.join(saved_path, f\"*_data{suffix}\"))\n",
    "    logger.info(\"Start to merge %d filles.\" % len(data_files))\n",
    "    all_df = []\n",
    "    for d in tqdm(data_files[:]):\n",
    "        try:\n",
    "            df_data = pd.read_csv(d)\n",
    "            print(d)\n",
    "            df_data = df_data.fillna(\"\")\n",
    "            df_data = refine_data(df_data)\n",
    "\n",
    "            df_merged = df_data        \n",
    "\n",
    "            # process places file\n",
    "            places_csv = d.replace(\"data.csv\", \"includes_places.csv\")\n",
    "            if os.path.exists(places_csv):\n",
    "                df_places = pd.read_csv(places_csv).fillna(\"\")\n",
    "                new_column_name = {name: \"places_table_\" + name for name in df_places.columns}\n",
    "                df_places = df_places.rename(columns=new_column_name)        \n",
    "                df_merged = pd.merge(df_merged, df_places, how='left', left_on=\"place_id\", right_on=\"places_table_id\")\n",
    "\n",
    "            # process tweets file\n",
    "            tweets_csv = d.replace(\"data.csv\", \"includes_tweets.csv\")\n",
    "            if os.path.exists(tweets_csv):\n",
    "                df_tweets = pd.read_csv(tweets_csv).fillna(\"\")\n",
    "                df_tweets[\"text\"] = df_tweets[\"text\"].str.replace(\"\\n\", \" \")\n",
    "                new_column_name = {name: \"tweets_table_\" + name for name in df_tweets.columns}\n",
    "                df_tweets = df_tweets.rename(columns=new_column_name)      \n",
    "                df_merged = pd.merge(df_merged, df_tweets, how='left', left_on=\"id\", right_on=\"tweets_table_id\")\n",
    "\n",
    "            # process users file\n",
    "            users_csv = d.replace(\"data.csv\", \"includes_users.csv\")\n",
    "            if os.path.exists(tweets_csv):\n",
    "                df_users = pd.read_csv(users_csv).fillna(\"\")\n",
    "                df_users[\"description\"] = df_users[\"description\"].str.replace(\"\\n\", \" \")\n",
    "                new_column_name = {name: \"users_table_\" + name for name in df_users.columns}\n",
    "                df_users = df_users.rename(columns=new_column_name)     \n",
    "                df_merged = pd.merge(df_merged, df_users, how='left', left_on=\"author_id\", right_on=\"users_table_id\")        \n",
    "\n",
    "            # process media file\n",
    "            media_csv = d.replace(\"data.csv\", \"includes_media.csv\")\n",
    "            if os.path.exists(media_csv):\n",
    "                df_media = pd.read_csv(media_csv).fillna(\"\")\n",
    "                df_media['media_key'] = df_media['media_key'].astype(str)\n",
    "                new_column_name = {name: \"media_table_\" + name for name in df_media.columns}\n",
    "                df_media = df_media.rename(columns=new_column_name)  \n",
    "                df_merged[\"media_table_rows\"] = df_merged.apply(find_media_row, args=(df_media,), axis=1)\n",
    "\n",
    "           # process poll file\n",
    "            poll_csv = d.replace(\"data.csv\", \"includes_polls.csv\")\n",
    "            if os.path.exists(poll_csv):\n",
    "                df_poll = pd.read_csv(poll_csv).fillna(\"\")\n",
    "                df_poll['poll_ids'] = df_poll['poll_ids'].astype(str)\n",
    "                new_column_name = {name: \"poll_table_\" + name for name in df_poll.columns}\n",
    "                df_poll = df_poll.rename(columns=new_column_name)  \n",
    "                df_merged[\"poll_table_rows\"] = df_merged.apply(find_poll_row, args=(poll_csv,), axis=1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Error in merge_results for loop: \", e)\n",
    "            logger.error(e, exc_info=True)\n",
    "        \n",
    "\n",
    "        df_merged = df_merged.fillna(\"\")\n",
    "        df_merged.replace(\"\\n\", \" \")\n",
    "        df_merged = df_merged.drop_duplicates(subset=['id'], keep='last')\n",
    "        # print(len(df_places))\n",
    "        # return df_merged\n",
    "        all_df.append(df_merged)\n",
    "\n",
    "    print(\"\\nGenerating final CSV file, including %d small CSV files.\" % len(all_df))\n",
    "    print(\"\\nPlease wait...\")\n",
    "\n",
    "    final_df = pd.concat(all_df).fillna(\"\")\n",
    "    final_df = final_df.apply(get_lonlat, axis=1).reset_index()\n",
    "    final_file = os.path.join(saved_path, \"merged.csv\")\n",
    "    final_df.to_csv(final_file, index=False)\n",
    "    logger.info(\"\\nSaved merged tweets in %s .\" % final_file)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52197395-1710-4a7e-a2e5-168f9c1ed5ba",
   "metadata": {},
   "source": [
    "# Set tokens\n",
    "\n",
    "Put your Twitter API tokens in the ```tweet_api_keys.txt``` file in the same directory of this notebook in the following format:\n",
    "```\n",
    "Consumer API Key: XXXX\n",
    "Consumer API Secret Key: XXXX\n",
    "Bearer Token: XXXX\n",
    "Access Token: XXXX\n",
    "Access Token Secret: XXXX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25925ca4-e4cc-4945-bba3-0eef63d6f79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_api_token(token_path):    \n",
    "    try:\n",
    "        with open(token_path, \"r\") as f:\n",
    "            logger.debug(\"token_path: %s\" % token_path)\n",
    "            lines = f.readlines()\n",
    "            logger.debug(\"lines in the file: %s\" % lines)\n",
    "\n",
    "            lines = [line.split(\": \")[-1][:-1] for line in lines]\n",
    "        return lines\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error: %s\" % str(e))\n",
    "\n",
    "\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def connect_to_endpoint(endpoint , headers, params):\n",
    "    response = requests.request(\"GET\", endpoint, headers=headers, params=params)\n",
    "    # print(response.status_code)\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(response.status_code, response.text)   \n",
    "    return response.json()\n",
    "\n",
    "token_path = r'J:\\Research\\tweet_download\\tweet_api_keys.txt'\n",
    "token_path = r'K:\\Research\\tweet_downloading\\python_code\\tweet_api_keys.txt'\n",
    "\n",
    "tokens = get_api_token(token_path)\n",
    "\n",
    "consumer_key = tokens[0]\n",
    "consumer_secret = tokens[1]\n",
    "bearer_token = tokens[2]\n",
    "access_token = tokens[3]\n",
    "access_token_secret = tokens[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f325a-ea4b-478f-95c1-68aff4a610f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Count tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49caac7d-ccf8-4e29-8f5e-9495339ceeac",
   "metadata": {
    "tags": []
   },
   "source": [
    "See the API document:\n",
    "https://developer.twitter.com/en/docs/twitter-api/tweets/counts/quick-start/recent-tweet-counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8a53b0cb-67d0-4e22-8ffd-488b4ccebb67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting tweets, please wait...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "310211"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_tweet_count(query, start_time, end_time, granularity='day', next_token=None):\n",
    "    print(\"Counting tweets, please wait...\")\n",
    "    \n",
    "    start_timer = time.perf_counter()  \n",
    "    \n",
    "    tweet_count_total = 0\n",
    "    endpoint = r'https://api.twitter.com/2/tweets/counts/all'\n",
    "    query_params = {'query': query, \\\n",
    "                    \"start_time\": start_time, \\\n",
    "                    \"end_time\": end_time, \\\n",
    "                    \"granularity\": granularity, \\\n",
    "                    \"next_token\": next_token, \\\n",
    "                    }\n",
    "    headers = create_headers(bearer_token)\n",
    "    \n",
    "    next_token = 'Start'\n",
    "    \n",
    "    page_cnt = 0\n",
    "    \n",
    "    while next_token is not None:\n",
    "        json_response = connect_to_endpoint(endpoint, headers, query_params)\n",
    "        is_too_many, elapsed_time = is_too_many_requests(json_response, start_timer)\n",
    "        next_token = json_response['meta'].get('next_token', None)        \n",
    "        tweet_count = json_response['meta']['total_tweet_count']\n",
    "        tweet_count_total += tweet_count\n",
    "        page_cnt += 1\n",
    "        query_params['next_token'] = next_token\n",
    "        \n",
    "        if page_cnt % 20 == 0:\n",
    "            print(f\"    current tweet count: {tweet_count_total}\")\n",
    "        \n",
    "#         print(f\"next_token: {next_token}. total_tweet_count: {tweet_count_total}\")\n",
    "    \n",
    "    return tweet_count_total#, json_response\n",
    "\n",
    "query = \"telemedicine  OR telehealth  OR telecare \"\n",
    "\n",
    "start_time = \"2019-01-01T00:00:00Z\"\n",
    "end_time   = \"2019-12-31T23:59:59Z\"\n",
    "\n",
    "tweet_count_total= get_tweet_count(query=query, start_time=start_time, end_time=end_time, granularity='day')\n",
    "tweet_count_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd7fdc-7c62-4786-95fa-f2993c53c70f",
   "metadata": {},
   "source": [
    "# Download tweets\n",
    "\n",
    "The following cell is a exmaple query to download tweets in Australia with a keyword of \"vaccine\" since 2021-01-01 to 2021-06-01.\n",
    "\n",
    "Please set ```query```, ```start_time```, ```end_time```, ```saved_path```, and ```max_results``` (10 - 500).\n",
    "\n",
    "See these pages to building a query: \n",
    "\n",
    "[Building queries for Search Tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#examples)\n",
    "\n",
    "[Search Tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a801f08-d288-4c9b-92da-a7d700319b69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a exmaple query to download tweets in Australia with a keyword of \"vaccine\" since 2020-01-01\n",
    "# keyword = \"vaccine\"\n",
    "\n",
    "# query = f\"({keyword}) place_country:AU -is:retweet\"\n",
    "# query = f\"({keyword}) place_country:AU\"\n",
    "# query = \"(vaccin OR vaccination OR vaccine OR vaccinate) place_country:AU\"\n",
    "\n",
    "query = \"telemedicine  OR telehealth  OR telecare\"\n",
    "\n",
    "# query = f\"({keyword})\"\n",
    "start_time = \"2019-01-01T00:00:00Z\"\n",
    "end_time   = \"2019-12-31T23:59:59Z\"\n",
    "max_results = 500   # max_results can be 500 if do not request the field: context_annotations\n",
    "\n",
    "# since_id = \"139819805172285849\"  # cannot used with start/end_time!\n",
    "\n",
    "\n",
    "# borrow from Twitter:\n",
    "# https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/master/Full-Archive-Search/full-archive-search.py\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "\n",
    "# saved_path = os.path.join(os.getcwd(), \"saved_tweets\")\n",
    "saved_path = r\"downloaded_tweets_test\"\n",
    "os.makedirs(saved_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "539aafaf-f394-4886-a5d2-c9eaa372cdaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting tweets, please wait...\n",
      "Found 441 tweets for query: telemedicine  OR telehealth  OR telecare. Period: 2021-11-29T20:00:01Z - 2021-11-30T00:00:01Z\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved 436 tweets in: K:\\Research\\tweet_downloading\\python_code\\downloaded_tweets_test\\1465410276778553347_1465470516379021313_436_data.csv\n",
      "Downloaded 436 tweets in total.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No next page! Exit.\n"
     ]
    }
   ],
   "source": [
    "# def create_headers(bearer_token):\n",
    "#     headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "#     return headers\n",
    "\n",
    "\n",
    "# def connect_to_endpoint(endpoint , headers, params):\n",
    "#     response = requests.request(\"GET\", endpoint, headers=headers, params=params)\n",
    "#     # print(response.status_code)\n",
    "# #     if response.status_code != 200:\n",
    "# #         raise Exception(response.status_code, response.text)   \n",
    "#     return response.json()\n",
    "\n",
    "def save_search(json_response, \n",
    "                saved_path,\n",
    "               is_zipped=False,\n",
    "               ):\n",
    "    try:\n",
    "        if not os.path.exists(saved_path):\n",
    "            os.mkdir(saved_path)\n",
    "            \n",
    "        if is_zipped:\n",
    "            suffix = '.csv.gz'\n",
    "        else:\n",
    "            suffix = '.csv'\n",
    "            \n",
    "\n",
    "        meta = json_response['meta']\n",
    "        data = json_response['data']\n",
    "        includes = json_response['includes']\n",
    "        basename = f\"{meta['oldest_id']}_{meta['newest_id']}_{meta['result_count']}\"\n",
    "\n",
    "        data_filename = os.path.join(saved_path, basename + f\"_data{suffix}\")\n",
    "        df = pd.DataFrame(data)\n",
    "        for c in df.columns:\n",
    "            df[c] = df[c].astype(str)\n",
    "            df[c] = df[c].str.replace('\\n',' ').replace(\",\", \";\").replace('\\r', '').replace('\\t', ' ').str.strip()\n",
    "        df.to_csv(data_filename, index=False)\n",
    "        result_count = meta['result_count']\n",
    "        result_count = str(result_count)\n",
    "        logger.info(\"Saved %s tweets in: %s\" % (result_count, data_filename))\n",
    "\n",
    "        for key in includes.keys():\n",
    "            includes_filename = os.path.join(saved_path, basename + f\"_includes_{key}{suffix}\")\n",
    "            df = pd.DataFrame(includes[key])\n",
    "            for c in df.columns:\n",
    "                df[c] = df[c].astype(str)\n",
    "                df[c] = df[c].str.replace('\\n',' ').replace(\",\", \";\").replace('\\r', '').replace('\\t', ' ').str.strip()\n",
    "            df.to_csv(includes_filename, index=False)\n",
    "            \n",
    "#         return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(e, exc_info=True)\n",
    "\n",
    "def execute_download(saved_path=os.getcwd(),\n",
    "                     \n",
    "                    \n",
    "                    ):\n",
    "    \n",
    "    max_results = 500\n",
    "    \n",
    "    chunk_size = 100000 # tweets\n",
    "    \n",
    "    has_context_annotations = False\n",
    "    \n",
    "    start_timer = time.perf_counter()\n",
    "\n",
    "    next_token = 'start'\n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "    headers = create_headers(bearer_token)\n",
    "    total = 0\n",
    "    query_params = {'query': query, \\\n",
    "                    \"max_results\": str(max_results), \\\n",
    "                    'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id', \\\n",
    "                    \n",
    "                     # HAVE context_annotations, max_results can be only 100\n",
    "#                     'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld', \\\n",
    "                    \n",
    "                    # NO context_annotations,  max_results can be 500\n",
    "                    'tweet.fields': 'attachments,author_id,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld', \\\n",
    "\n",
    "                    'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type', \\\n",
    "                    \"user.fields\": 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\\\n",
    "                    \"media.fields\": \"duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics\", \\\n",
    "                    \"poll.fields\": \"duration_minutes,end_datetime,id,options,voting_status\", \\\n",
    "                    \"start_time\": start_time, \\\n",
    "                    \"end_time\": end_time, \\\n",
    "                    # \"since_id\":since_id, \\  # cannot used with start/end_time!\n",
    "                    }\n",
    "    \n",
    "    if has_context_annotations:\n",
    "        query_params['tweet.fields'] = query_params['tweet.fields'] + \",context_annotations\"\n",
    "        if max_results > 100:\n",
    "            print(f\"max_results has set to 100 when requesting context_annotations. \")\n",
    "            \n",
    "    \n",
    "    tweet_count_total = get_tweet_count(query, start_time, end_time, granularity='day', next_token=None)\n",
    "    print(f\"Found {tweet_count_total} tweets for query: {query}. Period: {start_time} - {end_time}\")\n",
    "    \n",
    "    while next_token != \"\":\n",
    "        try:           \n",
    "            json_response = connect_to_endpoint(search_url, headers, query_params)\n",
    "            is_too_many, elapsed_time = is_too_many_requests(json_response, start_timer)\n",
    "#             df = pd.DataFrame(json_response['data'])\n",
    "            save_search(json_response, saved_path)\n",
    "            \n",
    "            total += int(json_response['meta']['result_count'])\n",
    "            logger.info(\"Downloaded %s tweets in total.\" % total)\n",
    "\n",
    "\n",
    "            next_token = json_response['meta'].get('next_token', \"\")\n",
    "            if next_token == \"\":\n",
    "                print(\"No next page! Exit.\")\n",
    "                return\n",
    "\n",
    "            query_params.update({\"next_token\": next_token})            \n",
    "#             time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(e, exc_info=True)\n",
    "            \n",
    "            print(e)\n",
    "            \n",
    "            now = time.perf_counter()\n",
    "            \n",
    "            time_window = 15 * 60 # seconds\n",
    "            \n",
    "            if 'Too Many Requests' in json_response.text:\n",
    "                elapsed_time = int(now - start_timer)\n",
    "                need_to_wait_time = time_window - elapsed_time\n",
    "                print(f'Too Many Requests, waiting for {need_to_wait_time} seconds.')\n",
    "                time.sleep(need_to_wait_time)\n",
    "                \n",
    "            continue\n",
    "\n",
    "saved_path = r'K:\\Research\\tweet_downloading\\python_code\\downloaded_tweets_test'\n",
    "execute_download(saved_path=saved_path)\n",
    "# merge_df = merge_results(saved_path)\n",
    "# merge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d78c7c-a848-4d10-ac30-e731f253a85e",
   "metadata": {},
   "source": [
    "# Testing merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd89a3a2-2102-4d43-93fb-fea3617a846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start to merge 1 filles.\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]Object of type Series is not JSON serializable\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\N\\AppData\\Local\\Temp/ipykernel_35216/1153263485.py\", line 188, in merge_results\n",
      "    df_merged[\"media_table_rows\"] = df_merged.apply(find_media_row, args=(df_media,), axis=1)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\", line 7768, in apply\n",
      "    return op.get_result()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\", line 185, in get_result\n",
      "    return self.apply_standard()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\", line 276, in apply_standard\n",
      "    results, res_index = self.apply_series_generator()\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\", line 290, in apply_series_generator\n",
      "    results[i] = self.f(v)\n",
      "  File \"D:\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\", line 110, in f\n",
      "    return func(x, *args, **kwds)\n",
      "  File \"C:\\Users\\N\\AppData\\Local\\Temp/ipykernel_35216/1153263485.py\", line 89, in find_media_row\n",
      "    row = json.dumps(row)\n",
      "  File \"D:\\Anaconda3\\lib\\json\\__init__.py\", line 231, in dumps\n",
      "    return _default_encoder.encode(obj)\n",
      "  File \"D:\\Anaconda3\\lib\\json\\encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"D:\\Anaconda3\\lib\\json\\encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"D:\\Anaconda3\\lib\\json\\encoder.py\", line 179, in default\n",
      "    raise TypeError(f'Object of type {o.__class__.__name__} '\n",
      "TypeError: Object of type Series is not JSON serializable\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K:\\Research\\tweet_downloading\\python_code\\downloaded_tweets_test\\1465410276778553347_1465470516379021313_436_data.csv\n",
      "3_1465467373977223171\n",
      "0     3_1465467373977223171\n",
      "1     3_1465467107760652301\n",
      "2     3_1465465488297844738\n",
      "3     3_1465464733759275012\n",
      "4     3_1465464266773966856\n",
      "              ...          \n",
      "57    3_1465410510749380612\n",
      "58    3_1465410510950703109\n",
      "59    3_1465410292305825797\n",
      "60    3_1465410276359086095\n",
      "61    3_1465410274786263045\n",
      "Name: media_table_media_key, Length: 62, dtype: object\n",
      "Error in merge_results for loop:  Object of type Series is not JSON serializable\n",
      "\n",
      "Generating final CSV file, including 1 small CSV files.\n",
      "\n",
      "Please wait...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'K:\\\\Research\\\\tweet_downloading\\\\python_code\\\\downloaded_tweets_test\\\\merged.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35216/2788203938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# execute_download(saved_path=saved_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmerge_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmerge_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_35216/1153263485.py\u001b[0m in \u001b[0;36mmerge_results\u001b[1;34m(saved_path, is_zipped)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[0mfinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_lonlat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[0mfinal_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"merged.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mfinal_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nSaved merged tweets in %s .\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfinal_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3401\u001b[0m             \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3402\u001b[0m             \u001b[0mescapechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mescapechar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3403\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3404\u001b[0m         )\n\u001b[0;32m   3405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m         ) as handles:\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'K:\\\\Research\\\\tweet_downloading\\\\python_code\\\\downloaded_tweets_test\\\\merged.csv'"
     ]
    }
   ],
   "source": [
    "# execute_download(saved_path=saved_path)\n",
    "merge_df = merge_results(saved_path)\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62db2f00-40a8-4c61-90a9-7023b5cb94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = 'start'\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "headers = create_headers(bearer_token)\n",
    "total = 0\n",
    "query_params = {'query': query, \\\n",
    "                \"max_results\": str(max_results), \\\n",
    "                'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id', \\\n",
    "                'tweet.fields': 'attachments,author_id,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld', \\\n",
    "                'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type', \\\n",
    "                \"user.fields\": 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\\\n",
    "                \"media.fields\": \"duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics\", \\\n",
    "                \"poll.fields\": \"duration_minutes,end_datetime,id,options,voting_status\", \\\n",
    "                \"start_time\": start_time, \\\n",
    "                \"end_time\": end_time, \\\n",
    "                # \"since_id\":since_id, \\  # cannot used with start/end_time!\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9fb53b59-a406-4a94-89e8-f62edbad81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = connect_to_endpoint(search_url, headers, query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "780fb3b7-f1d4-41c8-bf4c-6f07cfac8590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'newest_id': '1465470516379021313',\n",
       " 'oldest_id': '1465410276778553347',\n",
       " 'result_count': 435}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#json_response.keys()   # ['data', 'includes', 'meta']\n",
    "json_response['meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3164a63b-3008-49c1-9e55-1a0640f3a055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'tweets', 'media', 'places', 'polls'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response['includes'].keys()  # dict_keys(['users', 'tweets', 'media', 'places', 'polls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "caf50411-a465-4356-b522-f149e8b8023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_response['includes']['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c448cd9-6a8a-4457-a0a4-51a0b20053d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f49ffcd-11b4-4680-b20c-aa9c63194332",
   "metadata": {},
   "source": [
    "# Check merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f8c9564-1fae-429c-9ec8-9916099d2aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3170: DtypeWarning: Columns (0,1,27,35,36) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3170: DtypeWarning: Columns (0,1,27,35) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "csv_file1 = r'K:\\Research\\tweet_downloading\\telecare_all.csv'\n",
    "csv_file2 = r'K:\\Research\\tweet_downloading\\telecare_all2.csv'\n",
    "# csv_file = r'K:\\Research\\tweet_downloading\\python_code\\downloaded_tweets_test\\merged.csv'\n",
    "df1 = pd.read_csv(csv_file1)\n",
    "df2 = pd.read_csv(csv_file2)\n",
    "\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec325b8-b226-48f5-a329-d9ca511650b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08aea05c-ab7a-4fe0-ac90-ccc39916ebe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2019]), array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), 264150)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df2['year'] = df2['year'].astype(float, errors='ignore').astype(int, errors='ignore')\n",
    "df2['month'] = df2['month'].astype(float,errors='ignore').astype(int, errors='ignore')\n",
    "\n",
    "df2['year'] = pd.to_numeric(df2['year'],errors='coerce').replace(np.nan, 0, regex=True)\n",
    "df2['month'] = pd.to_numeric(df2['month'],errors='coerce').replace(np.nan, 0, regex=True)\n",
    "\n",
    "df2 = df2[df2['year'] > 2018]\n",
    "df2 = df2[df2['year'] < 2022]\n",
    "df2 = df2[df2['month'] > 0]\n",
    "df2 = df2[df2['month'] < 13]\n",
    "\n",
    "df2['year'].unique(), df2['month'].unique(), len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b17747cd-1306-48dc-8181-21df63a66f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2019., 2020., 2021.]),\n",
       " array([11., 12.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n",
       " 2312260)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df1['year'] = df1['year'].astype(float, errors='ignore').astype(int, errors='ignore')\n",
    "df1['month'] = df1['month'].astype(float,errors='ignore').astype(int, errors='ignore')\n",
    "\n",
    "df1['year'] = pd.to_numeric(df1['year'],errors='coerce').replace(np.nan, 0, regex=True)\n",
    "df1['month'] = pd.to_numeric(df1['month'],errors='coerce').replace(np.nan, 0, regex=True)\n",
    "\n",
    "df1 = df1[df1['year'] > 2018]\n",
    "df1 = df1[df1['year'] < 2022]\n",
    "df1 = df1[df1['month'] > 0]\n",
    "df1 = df1[df1['month'] < 13]\n",
    "\n",
    "df1['year'].unique(), df1['month'].unique(), len(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "504931a8-a568-4fdb-a323-6f5e6128e306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2576410"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.concat([df1, df2], axis=0)\n",
    "len(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57f458ae-e2a5-4b1e-877b-9f0f5c6a4d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['year'] = df_all['year'].astype(int)\n",
    "df_all['month'] = df_all['month'].astype(int)\n",
    "\n",
    "df_all.to_csv(r'K:\\Research\\tweet_downloading\\telecar.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ff04255-2e8c-457b-a45f-3ad6b2bd4f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3170: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>postdate</th>\n",
       "      <th>message</th>\n",
       "      <th>geoType</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>place</th>\n",
       "      <th>placeBboxwest</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>geo</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country</th>\n",
       "      <th>tweet_lang</th>\n",
       "      <th>message_en</th>\n",
       "      <th>message_cn</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.192478e+18</td>\n",
       "      <td>8.630710e+17</td>\n",
       "      <td>dejaehyun</td>\n",
       "      <td>2019-11-07T16:25:11.000Z</td>\n",
       "      <td>RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.192478e+18</td>\n",
       "      <td>1.304426e+08</td>\n",
       "      <td>diving_news</td>\n",
       "      <td>2019-11-07T16:25:05.000Z</td>\n",
       "      <td>American Well to buy Aligned Telehealth in beh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.192478e+18</td>\n",
       "      <td>3.301612e+09</td>\n",
       "      <td>jing2Inwza</td>\n",
       "      <td>2019-11-07T16:25:01.000Z</td>\n",
       "      <td>RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.192478e+18</td>\n",
       "      <td>2.376639e+09</td>\n",
       "      <td>ppcallmeh</td>\n",
       "      <td>2019-11-07T16:24:53.000Z</td>\n",
       "      <td>RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.192478e+18</td>\n",
       "      <td>1.128195e+18</td>\n",
       "      <td>Omar_Ziyadeh</td>\n",
       "      <td>2019-11-07T16:24:53.000Z</td>\n",
       "      <td>RT @AlignedTH: We are excited to announce that...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4003</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576405</th>\n",
       "      <td>1.192471e+18</td>\n",
       "      <td>6.027252e+08</td>\n",
       "      <td>_imlobo</td>\n",
       "      <td>2019-11-07T15:55:28.000Z</td>\n",
       "      <td>RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576406</th>\n",
       "      <td>1.192471e+18</td>\n",
       "      <td>1.367045e+09</td>\n",
       "      <td>ning_pb</td>\n",
       "      <td>2019-11-07T15:55:23.000Z</td>\n",
       "      <td>RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576407</th>\n",
       "      <td>1.192471e+18</td>\n",
       "      <td>9.588306e+17</td>\n",
       "      <td>DME_Health</td>\n",
       "      <td>2019-11-07T15:55:00.000Z</td>\n",
       "      <td>RT @_timos_: In Alaska; Telehealth is Popular ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576408</th>\n",
       "      <td>1.192471e+18</td>\n",
       "      <td>1.150765e+18</td>\n",
       "      <td>ilyeunsung</td>\n",
       "      <td>2019-11-07T15:54:58.000Z</td>\n",
       "      <td>RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576409</th>\n",
       "      <td>1.192470e+18</td>\n",
       "      <td>2.866715e+09</td>\n",
       "      <td>keinyma</td>\n",
       "      <td>2019-11-07T15:54:48.000Z</td>\n",
       "      <td>RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2576410 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetid        userid      username                  postdate  \\\n",
       "0        1.192478e+18  8.630710e+17     dejaehyun  2019-11-07T16:25:11.000Z   \n",
       "1        1.192478e+18  1.304426e+08   diving_news  2019-11-07T16:25:05.000Z   \n",
       "2        1.192478e+18  3.301612e+09    jing2Inwza  2019-11-07T16:25:01.000Z   \n",
       "3        1.192478e+18  2.376639e+09     ppcallmeh  2019-11-07T16:24:53.000Z   \n",
       "4        1.192478e+18  1.128195e+18  Omar_Ziyadeh  2019-11-07T16:24:53.000Z   \n",
       "...               ...           ...           ...                       ...   \n",
       "2576405  1.192471e+18  6.027252e+08       _imlobo  2019-11-07T15:55:28.000Z   \n",
       "2576406  1.192471e+18  1.367045e+09       ning_pb  2019-11-07T15:55:23.000Z   \n",
       "2576407  1.192471e+18  9.588306e+17    DME_Health  2019-11-07T15:55:00.000Z   \n",
       "2576408  1.192471e+18  1.150765e+18    ilyeunsung  2019-11-07T15:54:58.000Z   \n",
       "2576409  1.192470e+18  2.866715e+09       keinyma  2019-11-07T15:54:48.000Z   \n",
       "\n",
       "                                                   message geoType  longitude  \\\n",
       "0        RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...     NaN        NaN   \n",
       "1        American Well to buy Aligned Telehealth in beh...     NaN        NaN   \n",
       "2        RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...     NaN        NaN   \n",
       "3        RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...     NaN        NaN   \n",
       "4        RT @AlignedTH: We are excited to announce that...     NaN        NaN   \n",
       "...                                                    ...     ...        ...   \n",
       "2576405  RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...     NaN        NaN   \n",
       "2576406  RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...     NaN        NaN   \n",
       "2576407  RT @_timos_: In Alaska; Telehealth is Popular ...     NaN        NaN   \n",
       "2576408  RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...     NaN        NaN   \n",
       "2576409  RT @DrKehMDCU43: Telemedicine มันมาเร็วกว่าที่...     NaN        NaN   \n",
       "\n",
       "         latitude place  placeBboxwest  ...  year  month  geo country_code  \\\n",
       "0             NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "1             NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "2             NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "3             NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "4             NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "...           ...   ...            ...  ...   ...    ...  ...          ...   \n",
       "2576405       NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "2576406       NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "2576407       NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "2576408       NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "2576409       NaN   NaN            NaN  ...  2019     11  NaN          NaN   \n",
       "\n",
       "        country tweet_lang message_en  message_cn sentiment  topic  \n",
       "0           NaN         th        NaN         NaN    0.0000    NaN  \n",
       "1           NaN         en        NaN         NaN    0.2732    NaN  \n",
       "2           NaN         th        NaN         NaN    0.0000    NaN  \n",
       "3           NaN         th        NaN         NaN    0.0000    NaN  \n",
       "4           NaN         en        NaN         NaN    0.4003    NaN  \n",
       "...         ...        ...        ...         ...       ...    ...  \n",
       "2576405     NaN         th        NaN         NaN    0.0000    NaN  \n",
       "2576406     NaN         th        NaN         NaN    0.0000    NaN  \n",
       "2576407     NaN         en        NaN         NaN    0.4215    NaN  \n",
       "2576408     NaN         th        NaN         NaN    0.0000    NaN  \n",
       "2576409     NaN         th        NaN         NaN    0.0000    NaN  \n",
       "\n",
       "[2576410 rows x 45 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all2 = pd.read_csv(r'K:\\Research\\tweet_downloading\\telecar.csv.gz')\n",
    "\n",
    "df_all2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1157bc5-444c-4d77-b1fa-78d95d12ca9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2019, 2020, 2021], dtype=int64),\n",
       " array([11, 12,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int64),\n",
       " 2576410)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all2['year'].unique(), df_all2['month'].unique(), len(df_all2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "153e6550-42a0-481c-8f63-64fb95a3bcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tweetid</th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>postdate</th>\n",
       "      <th>message</th>\n",
       "      <th>geoType</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>place</th>\n",
       "      <th>placeBboxwest</th>\n",
       "      <th>...</th>\n",
       "      <th>bboxtype</th>\n",
       "      <th>placeid</th>\n",
       "      <th>geo</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country</th>\n",
       "      <th>tweet_lang</th>\n",
       "      <th>message_en</th>\n",
       "      <th>message_cn</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">2019</th>\n",
       "      <th>1</th>\n",
       "      <td>25114</td>\n",
       "      <td>25114</td>\n",
       "      <td>25114</td>\n",
       "      <td>25114</td>\n",
       "      <td>25114</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "      <td>25114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24386</td>\n",
       "      <td>24386</td>\n",
       "      <td>24386</td>\n",
       "      <td>24386</td>\n",
       "      <td>24386</td>\n",
       "      <td>443</td>\n",
       "      <td>443</td>\n",
       "      <td>443</td>\n",
       "      <td>443</td>\n",
       "      <td>443</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>443</td>\n",
       "      <td>443</td>\n",
       "      <td>443</td>\n",
       "      <td>443</td>\n",
       "      <td>24386</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26457</td>\n",
       "      <td>26457</td>\n",
       "      <td>26457</td>\n",
       "      <td>26457</td>\n",
       "      <td>26457</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>26457</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26457</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32316</td>\n",
       "      <td>32316</td>\n",
       "      <td>32316</td>\n",
       "      <td>32316</td>\n",
       "      <td>32316</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>534</td>\n",
       "      <td>534</td>\n",
       "      <td>533</td>\n",
       "      <td>533</td>\n",
       "      <td>32316</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26848</td>\n",
       "      <td>26848</td>\n",
       "      <td>26848</td>\n",
       "      <td>26848</td>\n",
       "      <td>26848</td>\n",
       "      <td>393</td>\n",
       "      <td>393</td>\n",
       "      <td>393</td>\n",
       "      <td>393</td>\n",
       "      <td>393</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>393</td>\n",
       "      <td>393</td>\n",
       "      <td>393</td>\n",
       "      <td>393</td>\n",
       "      <td>26848</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26848</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20657</td>\n",
       "      <td>20657</td>\n",
       "      <td>20657</td>\n",
       "      <td>20657</td>\n",
       "      <td>20657</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "      <td>20657</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25276</td>\n",
       "      <td>25276</td>\n",
       "      <td>25276</td>\n",
       "      <td>25276</td>\n",
       "      <td>25276</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>250</td>\n",
       "      <td>25276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25276</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24964</td>\n",
       "      <td>24964</td>\n",
       "      <td>24964</td>\n",
       "      <td>24964</td>\n",
       "      <td>24964</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>218</td>\n",
       "      <td>24964</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>23903</td>\n",
       "      <td>23903</td>\n",
       "      <td>23903</td>\n",
       "      <td>23903</td>\n",
       "      <td>23903</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>290</td>\n",
       "      <td>23903</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27928</td>\n",
       "      <td>27928</td>\n",
       "      <td>27928</td>\n",
       "      <td>27928</td>\n",
       "      <td>27928</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>341</td>\n",
       "      <td>27928</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>28710</td>\n",
       "      <td>28710</td>\n",
       "      <td>28710</td>\n",
       "      <td>28710</td>\n",
       "      <td>28710</td>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "      <td>28710</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28710</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20012</td>\n",
       "      <td>20012</td>\n",
       "      <td>20012</td>\n",
       "      <td>20012</td>\n",
       "      <td>20012</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>215</td>\n",
       "      <td>20012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"12\" valign=\"top\">2020</th>\n",
       "      <th>1</th>\n",
       "      <td>23397</td>\n",
       "      <td>23397</td>\n",
       "      <td>23397</td>\n",
       "      <td>23397</td>\n",
       "      <td>23397</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>263</td>\n",
       "      <td>23397</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23085</td>\n",
       "      <td>23085</td>\n",
       "      <td>23085</td>\n",
       "      <td>23085</td>\n",
       "      <td>23085</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>275</td>\n",
       "      <td>23085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>287911</td>\n",
       "      <td>287911</td>\n",
       "      <td>287910</td>\n",
       "      <td>287912</td>\n",
       "      <td>287912</td>\n",
       "      <td>3740</td>\n",
       "      <td>3740</td>\n",
       "      <td>3740</td>\n",
       "      <td>3740</td>\n",
       "      <td>3740</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3744</td>\n",
       "      <td>3740</td>\n",
       "      <td>3737</td>\n",
       "      <td>3737</td>\n",
       "      <td>287911</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>287912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>232768</td>\n",
       "      <td>232767</td>\n",
       "      <td>232767</td>\n",
       "      <td>232768</td>\n",
       "      <td>232768</td>\n",
       "      <td>3199</td>\n",
       "      <td>3199</td>\n",
       "      <td>3199</td>\n",
       "      <td>3199</td>\n",
       "      <td>3199</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3206</td>\n",
       "      <td>3199</td>\n",
       "      <td>3196</td>\n",
       "      <td>3197</td>\n",
       "      <td>232767</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232768</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>150085</td>\n",
       "      <td>150085</td>\n",
       "      <td>150085</td>\n",
       "      <td>150085</td>\n",
       "      <td>150085</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>1603</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1606</td>\n",
       "      <td>1603</td>\n",
       "      <td>1600</td>\n",
       "      <td>1600</td>\n",
       "      <td>150085</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>113354</td>\n",
       "      <td>113354</td>\n",
       "      <td>113354</td>\n",
       "      <td>113354</td>\n",
       "      <td>113354</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>1043</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1046</td>\n",
       "      <td>1043</td>\n",
       "      <td>1042</td>\n",
       "      <td>1042</td>\n",
       "      <td>113354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>116897</td>\n",
       "      <td>116899</td>\n",
       "      <td>116897</td>\n",
       "      <td>116899</td>\n",
       "      <td>116899</td>\n",
       "      <td>977</td>\n",
       "      <td>977</td>\n",
       "      <td>977</td>\n",
       "      <td>977</td>\n",
       "      <td>977</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>977</td>\n",
       "      <td>977</td>\n",
       "      <td>977</td>\n",
       "      <td>977</td>\n",
       "      <td>116897</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>116899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>125055</td>\n",
       "      <td>125055</td>\n",
       "      <td>125055</td>\n",
       "      <td>125055</td>\n",
       "      <td>125055</td>\n",
       "      <td>926</td>\n",
       "      <td>926</td>\n",
       "      <td>926</td>\n",
       "      <td>926</td>\n",
       "      <td>926</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>926</td>\n",
       "      <td>926</td>\n",
       "      <td>926</td>\n",
       "      <td>926</td>\n",
       "      <td>125055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>125055</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>88114</td>\n",
       "      <td>88114</td>\n",
       "      <td>88113</td>\n",
       "      <td>88114</td>\n",
       "      <td>88114</td>\n",
       "      <td>712</td>\n",
       "      <td>712</td>\n",
       "      <td>712</td>\n",
       "      <td>712</td>\n",
       "      <td>712</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>713</td>\n",
       "      <td>712</td>\n",
       "      <td>712</td>\n",
       "      <td>712</td>\n",
       "      <td>88113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>80123</td>\n",
       "      <td>80122</td>\n",
       "      <td>80122</td>\n",
       "      <td>80123</td>\n",
       "      <td>80123</td>\n",
       "      <td>593</td>\n",
       "      <td>593</td>\n",
       "      <td>593</td>\n",
       "      <td>593</td>\n",
       "      <td>593</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>593</td>\n",
       "      <td>593</td>\n",
       "      <td>593</td>\n",
       "      <td>593</td>\n",
       "      <td>80122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>59692</td>\n",
       "      <td>59692</td>\n",
       "      <td>59692</td>\n",
       "      <td>59692</td>\n",
       "      <td>59692</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>540</td>\n",
       "      <td>59692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>70884</td>\n",
       "      <td>70884</td>\n",
       "      <td>70881</td>\n",
       "      <td>70884</td>\n",
       "      <td>70884</td>\n",
       "      <td>522</td>\n",
       "      <td>522</td>\n",
       "      <td>522</td>\n",
       "      <td>522</td>\n",
       "      <td>522</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>523</td>\n",
       "      <td>522</td>\n",
       "      <td>522</td>\n",
       "      <td>522</td>\n",
       "      <td>70881</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70884</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">2021</th>\n",
       "      <th>1</th>\n",
       "      <td>75594</td>\n",
       "      <td>75594</td>\n",
       "      <td>75594</td>\n",
       "      <td>75594</td>\n",
       "      <td>75594</td>\n",
       "      <td>439</td>\n",
       "      <td>439</td>\n",
       "      <td>439</td>\n",
       "      <td>439</td>\n",
       "      <td>439</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>439</td>\n",
       "      <td>439</td>\n",
       "      <td>439</td>\n",
       "      <td>75594</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64193</td>\n",
       "      <td>64192</td>\n",
       "      <td>64192</td>\n",
       "      <td>64193</td>\n",
       "      <td>64193</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>447</td>\n",
       "      <td>64193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75381</td>\n",
       "      <td>75381</td>\n",
       "      <td>75381</td>\n",
       "      <td>75381</td>\n",
       "      <td>75381</td>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>75381</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75381</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85825</td>\n",
       "      <td>85825</td>\n",
       "      <td>85825</td>\n",
       "      <td>85825</td>\n",
       "      <td>85825</td>\n",
       "      <td>427</td>\n",
       "      <td>427</td>\n",
       "      <td>427</td>\n",
       "      <td>427</td>\n",
       "      <td>427</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>427</td>\n",
       "      <td>427</td>\n",
       "      <td>427</td>\n",
       "      <td>427</td>\n",
       "      <td>85825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100060</td>\n",
       "      <td>100060</td>\n",
       "      <td>100060</td>\n",
       "      <td>100060</td>\n",
       "      <td>100060</td>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "      <td>501</td>\n",
       "      <td>100060</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70023</td>\n",
       "      <td>70023</td>\n",
       "      <td>70023</td>\n",
       "      <td>70023</td>\n",
       "      <td>70023</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>339</td>\n",
       "      <td>70023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70023</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>156670</td>\n",
       "      <td>156670</td>\n",
       "      <td>156670</td>\n",
       "      <td>156670</td>\n",
       "      <td>156670</td>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>399</td>\n",
       "      <td>156670</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75889</td>\n",
       "      <td>75889</td>\n",
       "      <td>75889</td>\n",
       "      <td>75889</td>\n",
       "      <td>75889</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>335</td>\n",
       "      <td>75889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>83246</td>\n",
       "      <td>83244</td>\n",
       "      <td>83242</td>\n",
       "      <td>83248</td>\n",
       "      <td>83248</td>\n",
       "      <td>357</td>\n",
       "      <td>357</td>\n",
       "      <td>357</td>\n",
       "      <td>357</td>\n",
       "      <td>357</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>357</td>\n",
       "      <td>357</td>\n",
       "      <td>357</td>\n",
       "      <td>357</td>\n",
       "      <td>83246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83248</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>60357</td>\n",
       "      <td>60356</td>\n",
       "      <td>60356</td>\n",
       "      <td>60357</td>\n",
       "      <td>60357</td>\n",
       "      <td>336</td>\n",
       "      <td>336</td>\n",
       "      <td>336</td>\n",
       "      <td>336</td>\n",
       "      <td>336</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>336</td>\n",
       "      <td>336</td>\n",
       "      <td>336</td>\n",
       "      <td>60357</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>51231</td>\n",
       "      <td>51231</td>\n",
       "      <td>51230</td>\n",
       "      <td>51231</td>\n",
       "      <td>51231</td>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "      <td>289</td>\n",
       "      <td>51230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tweetid  userid  username  postdate  message  geoType  longitude  \\\n",
       "year month                                                                     \n",
       "2019 1        25114   25114     25114     25114    25114      344        344   \n",
       "     2        24386   24386     24386     24386    24386      443        443   \n",
       "     3        26457   26457     26457     26457    26457      447        447   \n",
       "     4        32316   32316     32316     32316    32316      534        534   \n",
       "     5        26848   26848     26848     26848    26848      393        393   \n",
       "     6        20657   20657     20657     20657    20657      316        316   \n",
       "     7        25276   25276     25276     25276    25276      250        250   \n",
       "     8        24964   24964     24964     24964    24964      218        218   \n",
       "     9        23903   23903     23903     23903    23903      290        290   \n",
       "     10       27928   27928     27928     27928    27928      341        341   \n",
       "     11       28710   28710     28710     28710    28710      327        327   \n",
       "     12       20012   20012     20012     20012    20012      215        215   \n",
       "2020 1        23397   23397     23397     23397    23397      263        263   \n",
       "     2        23085   23085     23085     23085    23085      275        275   \n",
       "     3       287911  287911    287910    287912   287912     3740       3740   \n",
       "     4       232768  232767    232767    232768   232768     3199       3199   \n",
       "     5       150085  150085    150085    150085   150085     1603       1603   \n",
       "     6       113354  113354    113354    113354   113354     1043       1043   \n",
       "     7       116897  116899    116897    116899   116899      977        977   \n",
       "     8       125055  125055    125055    125055   125055      926        926   \n",
       "     9        88114   88114     88113     88114    88114      712        712   \n",
       "     10       80123   80122     80122     80123    80123      593        593   \n",
       "     11       59692   59692     59692     59692    59692      540        540   \n",
       "     12       70884   70884     70881     70884    70884      522        522   \n",
       "2021 1        75594   75594     75594     75594    75594      439        439   \n",
       "     2        64193   64192     64192     64193    64193      447        447   \n",
       "     3        75381   75381     75381     75381    75381      426        426   \n",
       "     4        85825   85825     85825     85825    85825      427        427   \n",
       "     5       100060  100060    100060    100060   100060      501        501   \n",
       "     6        70023   70023     70023     70023    70023      339        339   \n",
       "     7       156670  156670    156670    156670   156670      399        399   \n",
       "     8        75889   75889     75889     75889    75889      335        335   \n",
       "     9        83246   83244     83242     83248    83248      357        357   \n",
       "     10       60357   60356     60356     60357    60357      336        336   \n",
       "     11       51231   51231     51230     51231    51231      289        289   \n",
       "\n",
       "            latitude  place  placeBboxwest  ...  bboxtype  placeid   geo  \\\n",
       "year month                                  ...                            \n",
       "2019 1           344    344            344  ...         0      344   344   \n",
       "     2           443    443            443  ...         0      443   443   \n",
       "     3           447    447            447  ...         0      447   447   \n",
       "     4           534    534            534  ...         0      534   534   \n",
       "     5           393    393            393  ...         0      393   393   \n",
       "     6           316    316            316  ...         0      316   316   \n",
       "     7           250    250            250  ...         0      250   250   \n",
       "     8           218    218            218  ...         0      218   218   \n",
       "     9           290    290            290  ...         0      290   290   \n",
       "     10          341    341            341  ...         0      341   341   \n",
       "     11          327    327            327  ...         0      327   327   \n",
       "     12          215    215            215  ...         0      215   215   \n",
       "2020 1           263    263            263  ...         0      263   263   \n",
       "     2           275    275            275  ...         0      275   275   \n",
       "     3          3740   3740           3740  ...         0     3744  3740   \n",
       "     4          3199   3199           3199  ...         0     3206  3199   \n",
       "     5          1603   1603           1603  ...         0     1606  1603   \n",
       "     6          1043   1043           1043  ...         0     1046  1043   \n",
       "     7           977    977            977  ...         0      977   977   \n",
       "     8           926    926            926  ...         0      926   926   \n",
       "     9           712    712            712  ...         0      713   712   \n",
       "     10          593    593            593  ...         0      593   593   \n",
       "     11          540    540            540  ...         0      540   540   \n",
       "     12          522    522            522  ...         0      523   522   \n",
       "2021 1           439    439            439  ...         0      439   439   \n",
       "     2           447    447            447  ...         0      447   447   \n",
       "     3           426    426            426  ...         0      426   426   \n",
       "     4           427    427            427  ...         0      427   427   \n",
       "     5           501    501            501  ...         0      501   501   \n",
       "     6           339    339            339  ...         0      339   339   \n",
       "     7           399    399            399  ...         0      399   399   \n",
       "     8           335    335            335  ...         0      335   335   \n",
       "     9           357    357            357  ...         0      357   357   \n",
       "     10          336    336            336  ...         0      336   336   \n",
       "     11          289    289            289  ...         0      289   289   \n",
       "\n",
       "            country_code  country  tweet_lang  message_en  message_cn  \\\n",
       "year month                                                              \n",
       "2019 1               344      344       25114           0           0   \n",
       "     2               443      443       24386           0           0   \n",
       "     3               447      447       26457           0           0   \n",
       "     4               533      533       32316           0           0   \n",
       "     5               393      393       26848           0           0   \n",
       "     6               315      315       20657           0           0   \n",
       "     7               250      250       25276           0           0   \n",
       "     8               218      218       24964           0           0   \n",
       "     9               290      290       23903           0           0   \n",
       "     10              341      341       27928           0           0   \n",
       "     11              327      327       28710           0           0   \n",
       "     12              215      215       20012           0           0   \n",
       "2020 1               263      263       23397           0           0   \n",
       "     2               275      275       23085           0           0   \n",
       "     3              3737     3737      287911           0           0   \n",
       "     4              3196     3197      232767           0           0   \n",
       "     5              1600     1600      150085           0           0   \n",
       "     6              1042     1042      113354           0           0   \n",
       "     7               977      977      116897           0           0   \n",
       "     8               926      926      125055           0           0   \n",
       "     9               712      712       88113           0           0   \n",
       "     10              593      593       80122           0           0   \n",
       "     11              540      540       59692           0           0   \n",
       "     12              522      522       70881           0           0   \n",
       "2021 1               439      439       75594           0           0   \n",
       "     2               447      447       64193           0           0   \n",
       "     3               426      426       75381           0           0   \n",
       "     4               427      427       85825           0           0   \n",
       "     5               501      501      100060           0           0   \n",
       "     6               339      339       70023           0           0   \n",
       "     7               399      399      156670           0           0   \n",
       "     8               335      335       75889           0           0   \n",
       "     9               357      357       83246           0           0   \n",
       "     10              336      336       60357           0           0   \n",
       "     11              289      289       51230           0           0   \n",
       "\n",
       "            sentiment  topic  \n",
       "year month                    \n",
       "2019 1          25114      0  \n",
       "     2          24386      0  \n",
       "     3          26457      0  \n",
       "     4          32316      0  \n",
       "     5          26848      0  \n",
       "     6          20657      0  \n",
       "     7          25276      0  \n",
       "     8          24964      0  \n",
       "     9          23903      0  \n",
       "     10         27928      0  \n",
       "     11         28710      0  \n",
       "     12         20012      0  \n",
       "2020 1          23397      0  \n",
       "     2          23085      0  \n",
       "     3         287912      0  \n",
       "     4         232768      0  \n",
       "     5         150085      0  \n",
       "     6         113354      0  \n",
       "     7         116899      0  \n",
       "     8         125055      0  \n",
       "     9          88114      0  \n",
       "     10         80123      0  \n",
       "     11         59692      0  \n",
       "     12         70884      0  \n",
       "2021 1          75594      0  \n",
       "     2          64193      0  \n",
       "     3          75381      0  \n",
       "     4          85825      0  \n",
       "     5         100060      0  \n",
       "     6          70023      0  \n",
       "     7         156670      0  \n",
       "     8          75889      0  \n",
       "     9          83248      0  \n",
       "     10         60357      0  \n",
       "     11         51231      0  \n",
       "\n",
       "[35 rows x 43 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all2.groupby(['year', 'month']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068c4be-448a-4ea6-97f7-8d6b21932242",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = ' male feminist.\n",
    " centre-left liberal.\n",
    " NeverBernie\n",
    " #StillWithHer'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
