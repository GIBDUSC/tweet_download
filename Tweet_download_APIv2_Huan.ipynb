{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210b0d9d-7687-4853-b593-280caae27568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: twarc in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: humanize>=3.9 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc) (3.13.1)\n",
      "Requirement already satisfied: requests-oauthlib>=1.3 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc) (1.3.0)\n",
      "Requirement already satisfied: tqdm>=4.62 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc) (4.62.3)\n",
      "Requirement already satisfied: click-plugins>=1 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc) (2.8.1)\n",
      "Requirement already satisfied: click<9,>=7 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc) (7.1.2)\n",
      "Requirement already satisfied: click-config-file>=0.6 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc) (0.6.0)\n",
      "Requirement already satisfied: configobj>=5.0.6 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from click-config-file>=0.6->twarc) (5.0.6)\n",
      "Requirement already satisfied: six in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from configobj>=5.0.6->click-config-file>=0.6->twarc) (1.15.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from humanize>=3.9->twarc) (3.10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests-oauthlib>=1.3->twarc) (3.1.1)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests-oauthlib>=1.3->twarc) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc) (2021.5.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from tqdm>=4.62->twarc) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from importlib-metadata->humanize>=3.9->twarc) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from importlib-metadata->humanize>=3.9->twarc) (3.10.0.0)\n",
      "Requirement already satisfied: twarc-csv in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: twarc>=2.8.0 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc-csv) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.59.0 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc-csv) (4.62.3)\n",
      "Requirement already satisfied: pandas>=1.2.5 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc-csv) (1.3.5)\n",
      "Requirement already satisfied: more-itertools>=8.7.0 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc-csv) (8.12.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from pandas>=1.2.5->twarc-csv) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from pandas>=1.2.5->twarc-csv) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from pandas>=1.2.5->twarc-csv) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.5->twarc-csv) (1.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from tqdm>=4.59.0->twarc-csv) (0.4.4)\n",
      "Requirement already satisfied: click-config-file>=0.6 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc>=2.8.0->twarc-csv) (0.6.0)\n",
      "Requirement already satisfied: requests-oauthlib>=1.3 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc>=2.8.0->twarc-csv) (1.3.0)\n",
      "Requirement already satisfied: humanize>=3.9 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc>=2.8.0->twarc-csv) (3.13.1)\n",
      "Requirement already satisfied: click<9,>=7 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc>=2.8.0->twarc-csv) (7.1.2)\n",
      "Requirement already satisfied: click-plugins>=1 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from twarc>=2.8.0->twarc-csv) (1.1.1)\n",
      "Requirement already satisfied: configobj>=5.0.6 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from click-config-file>=0.6->twarc>=2.8.0->twarc-csv) (5.0.6)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from humanize>=3.9->twarc>=2.8.0->twarc-csv) (3.10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests-oauthlib>=1.3->twarc>=2.8.0->twarc-csv) (3.1.1)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests-oauthlib>=1.3->twarc>=2.8.0->twarc-csv) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc>=2.8.0->twarc-csv) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc>=2.8.0->twarc-csv) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc>=2.8.0->twarc-csv) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc>=2.8.0->twarc-csv) (1.26.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from importlib-metadata->humanize>=3.9->twarc>=2.8.0->twarc-csv) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\gpu\\.conda\\envs\\google_street_view\\lib\\site-packages (from importlib-metadata->humanize>=3.9->twarc>=2.8.0->twarc-csv) (3.10.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install emoji --upgrade\n",
    "! pip install twarc\n",
    "! pip install twarc-csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62067fb5-8c1c-46dc-9e9b-1f74c67ac233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e79a2b2-9569-4fa6-abb4-76ded6166e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc2677c-34dc-4cae-91a6-da2352883fbf",
   "metadata": {},
   "source": [
    "# Prerequisite functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08fe033b-3786-4ca5-aaf6-1eb0e45cff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_logger(log_file_path=\"debug.log\", level=\"INFO\"):\n",
    "# def set_logger(log_file_path=\"debug.log\", level=\"DEBUG\"):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(level)\n",
    "    scream_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(log_file_path)\n",
    "    logger.addHandler(scream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "    \n",
    "\n",
    "\n",
    "try:\n",
    "    # print(len(logger.handlers))\n",
    "    while len(logger.handlers) > 1:\n",
    "        logger.handlers.pop(0)\n",
    "        # print(len(logger.handlers))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "logger = set_logger()\n",
    "\n",
    "def get_api_token(token_path):    \n",
    "    try:\n",
    "        with open(token_path, \"r\") as f:\n",
    "            logger.debug(\"token_path: %s\" % token_path)\n",
    "            lines = f.readlines()\n",
    "            logger.debug(\"lines in the file: %s\" % lines)\n",
    "\n",
    "            lines = [line.split(\": \")[-1][:-1] for line in lines]\n",
    "        return lines\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error: %s\" % str(e))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------- merge results -------------------#          \n",
    "def find_place_id(row):\n",
    "    # print(row)\n",
    "    \n",
    "    cell_text = row.get(\"geo\", \"\")\n",
    "    # print(cell_text)\n",
    "    if len(cell_text) > 1:\n",
    "        place_dict = ast.literal_eval(cell_text)\n",
    "    else:\n",
    "        return \"\"\n",
    "    # print(place_dict)\n",
    "    if isinstance(place_dict, dict):\n",
    "        place_id = place_dict.get(\"place_id\", \"\")\n",
    "        if len(place_id) > 1:\n",
    "            return place_id\n",
    "\n",
    "def clean_tweets(row):\n",
    "    \n",
    "    text = row['text'].replace('\\n',' ').replace(\",\", \";\").replace('\\r', '').replace('\\t', ' ').strip()\n",
    "    return text\n",
    "\n",
    "def refine_data(df):\n",
    "    df['place_id'] = df.apply(find_place_id, axis=1)\n",
    "    df['text'] = df.apply(clean_tweets, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def find_media_row(row, df_media):\n",
    "    cell_text = row[\"attachments\"]\n",
    "    if len(cell_text) > 1:\n",
    "        attachments_dict = ast.literal_eval(cell_text)\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "    if isinstance(attachments_dict, dict):\n",
    "        media_keys = attachments_dict.get(\"media_keys\", \"\")\n",
    "        media_rows = []\n",
    "        # print(df_media)\n",
    "        # print(attachments_dict)\n",
    "        for key in media_keys:\n",
    "            key = str(key)\n",
    "            if len(key) > 1:\n",
    "                # print(key)\n",
    "                # print(df_media['media_table_media_key'])\n",
    "                row = df_media[df_media['media_table_media_key']==key].iloc[0].to_json()\n",
    "                # print(df_media[df_media['media_table_media_key']==key])\n",
    "                media_rows.append(row)\n",
    "        # print(media_rows)\n",
    "        return media_rows\n",
    "    return \"\"\n",
    "\n",
    "def get_lonlat(row):\n",
    "    row[\"lon\"] = \"\"\n",
    "    row[\"lat\"] = \"\"\n",
    "#     print('row[places_table_geo]:', row[\"places_table_geo\"])\n",
    "    if len(row[\"places_table_geo\"]) > 1:\n",
    "        geo_dict = ast.literal_eval(row[\"places_table_geo\"])\n",
    "#         print('geo_dict:', geo_dict)\n",
    "        bbox = geo_dict.get(\"bbox\", [])\n",
    "        if len(bbox) == 4:\n",
    "            row[\"lon\"] = (bbox[0] + bbox[2]) / 2\n",
    "            row[\"lat\"] = (bbox[1] + bbox[3]) / 2\n",
    "    return row\n",
    "\n",
    "def merge_results(saved_path):\n",
    "    data_files = glob.glob(os.path.join(saved_path, \"*_data.csv\"))\n",
    "    logger.info(\"Start to merge %d filles.\" % len(data_files))\n",
    "    all_df = []\n",
    "    for d in tqdm(data_files[:]):\n",
    "        df_data = pd.read_csv(d)\n",
    "        print(d)\n",
    "        df_data = df_data.fillna(\"\")\n",
    "        df_data = refine_data(df_data)\n",
    "\n",
    "        df_merged = df_data        \n",
    "\n",
    "        # process places file\n",
    "        places_csv = d.replace(\"data.csv\", \"includes_places.csv\")\n",
    "        if os.path.exists(places_csv):\n",
    "            df_places = pd.read_csv(places_csv).fillna(\"\")\n",
    "            new_column_name = {name: \"places_table_\" + name for name in df_places.columns}\n",
    "            df_places = df_places.rename(columns=new_column_name)        \n",
    "            df_merged = pd.merge(df_merged, df_places, how='left', left_on=\"place_id\", right_on=\"places_table_id\")\n",
    "\n",
    "        # process tweets file\n",
    "        tweets_csv = d.replace(\"data.csv\", \"includes_tweets.csv\")\n",
    "        if os.path.exists(tweets_csv):\n",
    "            df_tweets = pd.read_csv(tweets_csv).fillna(\"\")\n",
    "            df_tweets[\"text\"] = df_tweets[\"text\"].str.replace(\"\\n\", \" \")\n",
    "            new_column_name = {name: \"tweets_table_\" + name for name in df_tweets.columns}\n",
    "            df_tweets = df_tweets.rename(columns=new_column_name)      \n",
    "            df_merged = pd.merge(df_merged, df_tweets, how='left', left_on=\"id\", right_on=\"tweets_table_id\")\n",
    "\n",
    "        # process users file\n",
    "        users_csv = d.replace(\"data.csv\", \"includes_users.csv\")\n",
    "        if os.path.exists(tweets_csv):\n",
    "            df_users = pd.read_csv(users_csv).fillna(\"\")\n",
    "            df_users[\"description\"] = df_users[\"description\"].str.replace(\"\\n\", \" \")\n",
    "            new_column_name = {name: \"users_table_\" + name for name in df_users.columns}\n",
    "            df_users = df_users.rename(columns=new_column_name)     \n",
    "            df_merged = pd.merge(df_merged, df_users, how='left', left_on=\"author_id\", right_on=\"users_table_id\")        \n",
    "\n",
    "        # process media file\n",
    "        media_csv = d.replace(\"data.csv\", \"includes_media.csv\")\n",
    "        if os.path.exists(media_csv):\n",
    "            df_media = pd.read_csv(media_csv).fillna(\"\")\n",
    "            df_media['media_key'] = df_media['media_key'].astype(str)\n",
    "            new_column_name = {name: \"media_table_\" + name for name in df_media.columns}\n",
    "            df_media = df_media.rename(columns=new_column_name)  \n",
    "            df_merged[\"media_table_rows\"] = df_merged.apply(find_media_row, args=(df_media,), axis=1)\n",
    "        \n",
    "\n",
    "        df_merged = df_merged.fillna(\"\")\n",
    "        df_merged.replace(\"\\n\", \" \")\n",
    "        df_merged = df_merged.drop_duplicates(subset=['id'], keep='last')\n",
    "        # print(len(df_places))\n",
    "        # return df_merged\n",
    "        all_df.append(df_merged)\n",
    "\n",
    "    print(\"\\nGenerating final CSV file, including %d small CSV files.\" % len(all_df))\n",
    "    print(\"\\nPlease wait...\")\n",
    "\n",
    "    final_df = pd.concat(all_df).fillna(\"\")\n",
    "    final_df = final_df.apply(get_lonlat, axis=1).reset_index()\n",
    "    final_file = os.path.join(saved_path, \"merged.csv.gz\")\n",
    "    final_df.to_csv(final_file, index=False)\n",
    "    logger.info(\"\\nSaved merged tweets in %s .\" % final_file)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52197395-1710-4a7e-a2e5-168f9c1ed5ba",
   "metadata": {},
   "source": [
    "# Set tokens\n",
    "\n",
    "Put your Twitter API tokens in the ```tweet_api_keys.txt``` file in the same directory of this notebook in the following format:\n",
    "```\n",
    "Consumer API Key: XXXX\n",
    "Consumer API Secret Key: XXXX\n",
    "Bearer Token: XXXX\n",
    "Access Token: XXXX\n",
    "Access Token Secret: XXXX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25925ca4-e4cc-4945-bba3-0eef63d6f79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_path = r'J:\\Research\\tweet_download\\tweet_api_keys.txt'\n",
    "\n",
    "tokens = get_api_token(token_path)\n",
    "\n",
    "consumer_key = tokens[0]\n",
    "consumer_secret = tokens[1]\n",
    "bearer_token = tokens[2]\n",
    "access_token = tokens[3]\n",
    "access_token_secret = tokens[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bd7fdc-7c62-4786-95fa-f2993c53c70f",
   "metadata": {},
   "source": [
    "# Download tweets\n",
    "\n",
    "The following cell is a exmaple query to download tweets in Australia with a keyword of \"vaccine\" since 2021-01-01 to 2021-06-01.\n",
    "\n",
    "Please set ```query```, ```start_time```, ```end_time```, ```saved_path```, and ```max_results``` (10 - 500).\n",
    "\n",
    "See these pages to building a query: \n",
    "\n",
    "[Building queries for Search Tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/build-a-query#examples)\n",
    "\n",
    "[Search Tweets](https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a801f08-d288-4c9b-92da-a7d700319b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a exmaple query to download tweets in Australia with a keyword of \"vaccine\" since 2020-01-01\n",
    "# keyword = \"vaccine\"\n",
    "\n",
    "# query = f\"({keyword}) place_country:AU -is:retweet\"\n",
    "# query = f\"({keyword}) place_country:AU\"\n",
    "# query = \"(vaccin OR vaccination OR vaccine OR vaccinate) place_country:AU\"\n",
    "\n",
    "query = \"telemedicine  OR telehealth  OR telecare\"\n",
    "\n",
    "# query = f\"({keyword})\"\n",
    "start_time = \"2021-11-29T20:00:01Z\"\n",
    "end_time = \"2021-11-30T00:00:01Z\"\n",
    "max_results = 100   # max_results can be 500 if do not request the field: context_annotations\n",
    "\n",
    "# since_id = \"139819805172285849\"  # cannot used with start/end_time!\n",
    "\n",
    "\n",
    "# borrow from Twitter:\n",
    "# https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/master/Full-Archive-Search/full-archive-search.py\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "\n",
    "# saved_path = os.path.join(os.getcwd(), \"saved_tweets\")\n",
    "saved_path = r\"downloaded_tweets_test\"\n",
    "os.makedirs(saved_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "539aafaf-f394-4886-a5d2-c9eaa372cdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved 100 tweets in: downloaded_tweets_test\\1465458465405648903_1465470516379021313_100_data.csv\n",
      "Downloaded 100 tweets in total.\n",
      "Saved 97 tweets in: downloaded_tweets_test\\1465443799593734146_1465458273432182785_97_data.csv\n",
      "Downloaded 197 tweets in total.\n",
      "Saved 99 tweets in: downloaded_tweets_test\\1465429190065463312_1465443617347039232_99_data.csv\n",
      "Downloaded 296 tweets in total.\n",
      "Saved 99 tweets in: downloaded_tweets_test\\1465419376308359170_1465428976810213382_99_data.csv\n",
      "Downloaded 395 tweets in total.\n",
      "Saved 40 tweets in: downloaded_tweets_test\\1465410276778553347_1465419287460519941_40_data.csv\n",
      "Downloaded 435 tweets in total.\n",
      "Start to merge 5 filles.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No next page! Exit.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:00<00:00, 19.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded_tweets_test\\1465410276778553347_1465419287460519941_40_data.csv\n",
      "downloaded_tweets_test\\1465419376308359170_1465428976810213382_99_data.csv\n",
      "downloaded_tweets_test\\1465429190065463312_1465443617347039232_99_data.csv\n",
      "downloaded_tweets_test\\1465443799593734146_1465458273432182785_97_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 17.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded_tweets_test\\1465458465405648903_1465470516379021313_100_data.csv\n",
      "\n",
      "Generating final CSV file, including 5 small CSV files.\n",
      "\n",
      "Please wait...\n",
      "row[places_table_geo]: nan\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-454c236c4f1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[0mexecute_download\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msaved_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[0mmerge_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[0mmerge_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-11dd695e3262>\u001b[0m in \u001b[0;36mmerge_results\u001b[1;34m(saved_path)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mfinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0mfinal_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_lonlat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0mfinal_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"merged.csv.gz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0mfinal_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Google_street_view\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8739\u001b[0m         )\n\u001b[1;32m-> 8740\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[1;32m~\\.conda\\envs\\Google_street_view\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Google_street_view\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Google_street_view\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-11dd695e3262>\u001b[0m in \u001b[0;36mget_lonlat\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lat\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'row[places_table_geo]:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"places_table_geo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"places_table_geo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[0mgeo_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"places_table_geo\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;31m#         print('geo_dict:', geo_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url, headers, params):\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "    # print(response.status_code)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(response.status_code, response.text)   \n",
    "    return response.json()\n",
    "\n",
    "def save_search(json_response, saved_path):\n",
    "    try:\n",
    "        if not os.path.exists(saved_path):\n",
    "            os.mkdir(saved_path)\n",
    "\n",
    "        meta = json_response['meta']\n",
    "        data = json_response['data']\n",
    "        includes = json_response['includes']\n",
    "        basename = f\"{meta['oldest_id']}_{meta['newest_id']}_{meta['result_count']}\"\n",
    "\n",
    "        data_filename = os.path.join(saved_path, basename + \"_data.csv\")\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(data_filename, index=False)\n",
    "        result_count = meta['result_count']\n",
    "        result_count = str(result_count)\n",
    "        logger.info(\"Saved %s tweets in: %s\" % (result_count, data_filename))\n",
    "\n",
    "        for key in includes.keys():\n",
    "            includes_filename = os.path.join(saved_path, basename + f\"_includes_{key}.csv\")\n",
    "            df = pd.DataFrame(includes[key])\n",
    "            df.to_csv(includes_filename, index=False)\n",
    "    except Exception as e:\n",
    "        logger.error(e, exc_info=True)\n",
    "\n",
    "def execute_download(saved_path=os.getcwd()):\n",
    "    \n",
    "    start_timer = time.perf_counter()\n",
    "\n",
    "    next_token = 'start'\n",
    "    search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "    headers = create_headers(bearer_token)\n",
    "    total = 0\n",
    "    query_params = {'query': query, \\\n",
    "                    \"max_results\": str(max_results), \\\n",
    "                    'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id', \\\n",
    "                    'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld', \\\n",
    "                    'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type', \\\n",
    "                    \"user.fields\": 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\\\n",
    "                    \"media.fields\": \"duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics\", \\\n",
    "                    \"poll.fields\": \"duration_minutes,end_datetime,id,options,voting_status\", \\\n",
    "                    \"start_time\": start_time, \\\n",
    "                    \"end_time\": end_time, \\\n",
    "                    # \"since_id\":since_id, \\  # cannot used with start/end_time!\n",
    "                    }\n",
    "\n",
    "    while next_token != \"\":\n",
    "        try:\n",
    "            \n",
    "            json_response = connect_to_endpoint(search_url, headers, query_params)\n",
    "#             df = pd.DataFrame(json_response['data'])\n",
    "            save_search(json_response, saved_path)\n",
    "            \n",
    "            total += int(json_response['meta']['result_count'])\n",
    "            logger.info(\"Downloaded %s tweets in total.\" % total)\n",
    "\n",
    "\n",
    "            next_token = json_response['meta'].get('next_token', \"\")\n",
    "            if next_token == \"\":\n",
    "                print(\"No next page! Exit.\")\n",
    "                return\n",
    "\n",
    "            query_params.update({\"next_token\": next_token})            \n",
    "#             time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(e, exc_info=True)\n",
    "            \n",
    "            print(e)\n",
    "            \n",
    "            now = time.perf_counter()\n",
    "            \n",
    "            time_window = 15 * 60 # seconds\n",
    "            \n",
    "            if 'Too Many Requests' in json_response.text:\n",
    "                elapsed_time = int(now - start_timer)\n",
    "                need_to_wait_time = time_window - elapsed_time\n",
    "                print(f'Too Many Requests, waiting for {need_to_wait_time} seconds.')\n",
    "                time.sleep(need_to_wait_time)\n",
    "                \n",
    "            continue\n",
    "\n",
    "\n",
    "execute_download(saved_path=saved_path)\n",
    "merge_df = merge_results(saved_path)\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd89a3a2-2102-4d43-93fb-fea3617a846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start to merge 5 filles.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 27.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloaded_tweets_test\\1465410276778553347_1465419287460519941_40_data.csv\n",
      "downloaded_tweets_test\\1465419376308359170_1465428976810213382_99_data.csv\n",
      "downloaded_tweets_test\\1465429190065463312_1465443617347039232_99_data.csv\n",
      "downloaded_tweets_test\\1465443799593734146_1465458273432182785_97_data.csv\n",
      "downloaded_tweets_test\\1465458465405648903_1465470516379021313_100_data.csv\n",
      "\n",
      "Generating final CSV file, including 5 small CSV files.\n",
      "\n",
      "Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved merged tweets in downloaded_tweets_test\\merged.csv.gz .\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>entities</th>\n",
       "      <th>source</th>\n",
       "      <th>reply_settings</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>author_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>...</th>\n",
       "      <th>geo</th>\n",
       "      <th>places_table_name</th>\n",
       "      <th>places_table_country_code</th>\n",
       "      <th>places_table_geo</th>\n",
       "      <th>places_table_country</th>\n",
       "      <th>places_table_place_type</th>\n",
       "      <th>places_table_id</th>\n",
       "      <th>places_table_full_name</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Telehealth takes off but security concerns per...</td>\n",
       "      <td>1465419287460519941</td>\n",
       "      <td>{'hashtags': [{'start': 51, 'end': 59, 'tag': ...</td>\n",
       "      <td>TwinyBots</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>1439265257894170630</td>\n",
       "      <td>2021-11-29T20:35:58.000Z</td>\n",
       "      <td>1465419287460519941</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>how #telepsychiatry saved this family #telemed...</td>\n",
       "      <td>1465419037652008961</td>\n",
       "      <td>{'hashtags': [{'start': 4, 'end': 19, 'tag': '...</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>511022548</td>\n",
       "      <td>2021-11-29T20:34:59.000Z</td>\n",
       "      <td>1465419037652008961</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @CaliforniaTRC: You won't want to miss this...</td>\n",
       "      <td>1465418791098093575</td>\n",
       "      <td>{'hashtags': [{'start': 126, 'end': 137, 'tag'...</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>437081670</td>\n",
       "      <td>2021-11-29T20:34:00.000Z</td>\n",
       "      <td>1465418791098093575</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>@audrey_earth Yep- The clinic my husband &amp;amp;...</td>\n",
       "      <td>1465418439804305418</td>\n",
       "      <td>{'mentions': [{'start': 0, 'end': 13, 'usernam...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>48845861</td>\n",
       "      <td>2021-11-29T20:32:36.000Z</td>\n",
       "      <td>1464963974571646976</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Gov. Noem pushes back on narrative GOP is anti...</td>\n",
       "      <td>1465418362708705283</td>\n",
       "      <td>{'mentions': [{'start': 137, 'end': 152, 'user...</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>19020689</td>\n",
       "      <td>2021-11-29T20:32:18.000Z</td>\n",
       "      <td>1465418362708705283</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>95</td>\n",
       "      <td>9 out of 10 Healthcare Organizations Provide T...</td>\n",
       "      <td>1465458929383751683</td>\n",
       "      <td>{'urls': [{'start': 121, 'end': 144, 'url': 'h...</td>\n",
       "      <td>IFTTT</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>369107103</td>\n",
       "      <td>2021-11-29T23:13:30.000Z</td>\n",
       "      <td>1465458929383751683</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>96</td>\n",
       "      <td>Gov. Noem pushes back on narrative GOP is anti...</td>\n",
       "      <td>1465458743806676995</td>\n",
       "      <td>{'annotations': [{'start': 0, 'end': 8, 'proba...</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>1362186522062110720</td>\n",
       "      <td>2021-11-29T23:12:46.000Z</td>\n",
       "      <td>1465458743806676995</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>97</td>\n",
       "      <td>RT @4fitnesshealthy: Comparing Video-Based; Te...</td>\n",
       "      <td>1465458518425751557</td>\n",
       "      <td>{'mentions': [{'start': 3, 'end': 19, 'usernam...</td>\n",
       "      <td>Healthymes Retweeter</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>2238159482</td>\n",
       "      <td>2021-11-29T23:11:52.000Z</td>\n",
       "      <td>1465458518425751557</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>98</td>\n",
       "      <td>RT @4fitnesshealthy: Telehealth-delivered diet...</td>\n",
       "      <td>1465458467141996547</td>\n",
       "      <td>{'annotations': [{'start': 112, 'end': 125, 'p...</td>\n",
       "      <td>Healthymes Retweeter</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>2238159482</td>\n",
       "      <td>2021-11-29T23:11:40.000Z</td>\n",
       "      <td>1465458467141996547</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>99</td>\n",
       "      <td>RT @KevinDTweets: “This investigation exposes ...</td>\n",
       "      <td>1465458465405648903</td>\n",
       "      <td>{'mentions': [{'start': 3, 'end': 16, 'usernam...</td>\n",
       "      <td>Twitter for iPad</td>\n",
       "      <td>everyone</td>\n",
       "      <td>False</td>\n",
       "      <td>237227508</td>\n",
       "      <td>2021-11-29T23:11:39.000Z</td>\n",
       "      <td>1465458465405648903</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>435 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text  \\\n",
       "0        0  Telehealth takes off but security concerns per...   \n",
       "1        1  how #telepsychiatry saved this family #telemed...   \n",
       "2        2  RT @CaliforniaTRC: You won't want to miss this...   \n",
       "3        3  @audrey_earth Yep- The clinic my husband &amp;...   \n",
       "4        4  Gov. Noem pushes back on narrative GOP is anti...   \n",
       "..     ...                                                ...   \n",
       "430     95  9 out of 10 Healthcare Organizations Provide T...   \n",
       "431     96  Gov. Noem pushes back on narrative GOP is anti...   \n",
       "432     97  RT @4fitnesshealthy: Comparing Video-Based; Te...   \n",
       "433     98  RT @4fitnesshealthy: Telehealth-delivered diet...   \n",
       "434     99  RT @KevinDTweets: “This investigation exposes ...   \n",
       "\n",
       "                      id                                           entities  \\\n",
       "0    1465419287460519941  {'hashtags': [{'start': 51, 'end': 59, 'tag': ...   \n",
       "1    1465419037652008961  {'hashtags': [{'start': 4, 'end': 19, 'tag': '...   \n",
       "2    1465418791098093575  {'hashtags': [{'start': 126, 'end': 137, 'tag'...   \n",
       "3    1465418439804305418  {'mentions': [{'start': 0, 'end': 13, 'usernam...   \n",
       "4    1465418362708705283  {'mentions': [{'start': 137, 'end': 152, 'user...   \n",
       "..                   ...                                                ...   \n",
       "430  1465458929383751683  {'urls': [{'start': 121, 'end': 144, 'url': 'h...   \n",
       "431  1465458743806676995  {'annotations': [{'start': 0, 'end': 8, 'proba...   \n",
       "432  1465458518425751557  {'mentions': [{'start': 3, 'end': 19, 'usernam...   \n",
       "433  1465458467141996547  {'annotations': [{'start': 112, 'end': 125, 'p...   \n",
       "434  1465458465405648903  {'mentions': [{'start': 3, 'end': 16, 'usernam...   \n",
       "\n",
       "                   source reply_settings  possibly_sensitive  \\\n",
       "0               TwinyBots       everyone               False   \n",
       "1         Twitter Web App       everyone               False   \n",
       "2         Twitter Web App       everyone               False   \n",
       "3     Twitter for Android       everyone               False   \n",
       "4      Twitter for iPhone       everyone               False   \n",
       "..                    ...            ...                 ...   \n",
       "430                 IFTTT       everyone               False   \n",
       "431    Twitter for iPhone       everyone               False   \n",
       "432  Healthymes Retweeter       everyone               False   \n",
       "433  Healthymes Retweeter       everyone               False   \n",
       "434      Twitter for iPad       everyone               False   \n",
       "\n",
       "               author_id                created_at      conversation_id  ...  \\\n",
       "0    1439265257894170630  2021-11-29T20:35:58.000Z  1465419287460519941  ...   \n",
       "1              511022548  2021-11-29T20:34:59.000Z  1465419037652008961  ...   \n",
       "2              437081670  2021-11-29T20:34:00.000Z  1465418791098093575  ...   \n",
       "3               48845861  2021-11-29T20:32:36.000Z  1464963974571646976  ...   \n",
       "4               19020689  2021-11-29T20:32:18.000Z  1465418362708705283  ...   \n",
       "..                   ...                       ...                  ...  ...   \n",
       "430            369107103  2021-11-29T23:13:30.000Z  1465458929383751683  ...   \n",
       "431  1362186522062110720  2021-11-29T23:12:46.000Z  1465458743806676995  ...   \n",
       "432           2238159482  2021-11-29T23:11:52.000Z  1465458518425751557  ...   \n",
       "433           2238159482  2021-11-29T23:11:40.000Z  1465458467141996547  ...   \n",
       "434            237227508  2021-11-29T23:11:39.000Z  1465458465405648903  ...   \n",
       "\n",
       "    geo places_table_name places_table_country_code places_table_geo  \\\n",
       "0                                                                      \n",
       "1                                                                      \n",
       "2                                                                      \n",
       "3                                                                      \n",
       "4                                                                      \n",
       "..   ..               ...                       ...              ...   \n",
       "430                                                                    \n",
       "431                                                                    \n",
       "432                                                                    \n",
       "433                                                                    \n",
       "434                                                                    \n",
       "\n",
       "    places_table_country places_table_place_type places_table_id  \\\n",
       "0                                                                  \n",
       "1                                                                  \n",
       "2                                                                  \n",
       "3                                                                  \n",
       "4                                                                  \n",
       "..                   ...                     ...             ...   \n",
       "430                                                                \n",
       "431                                                                \n",
       "432                                                                \n",
       "433                                                                \n",
       "434                                                                \n",
       "\n",
       "    places_table_full_name lon lat  \n",
       "0                                   \n",
       "1                                   \n",
       "2                                   \n",
       "3                                   \n",
       "4                                   \n",
       "..                     ...  ..  ..  \n",
       "430                                 \n",
       "431                                 \n",
       "432                                 \n",
       "433                                 \n",
       "434                                 \n",
       "\n",
       "[435 rows x 56 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute_download(saved_path=saved_path)\n",
    "merge_df = merge_results(saved_path)\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62db2f00-40a8-4c61-90a9-7023b5cb94fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token = 'start'\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "headers = create_headers(bearer_token)\n",
    "total = 0\n",
    "query_params = {'query': query, \\\n",
    "                \"max_results\": str(max_results), \\\n",
    "                'expansions': 'attachments.poll_ids,attachments.media_keys,author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id', \\\n",
    "                'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld', \\\n",
    "                'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type', \\\n",
    "                \"user.fields\": 'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld',\\\n",
    "                \"media.fields\": \"duration_ms,height,media_key,preview_image_url,type,url,width,public_metrics\", \\\n",
    "                \"poll.fields\": \"duration_minutes,end_datetime,id,options,voting_status\", \\\n",
    "                \"start_time\": start_time, \\\n",
    "                \"end_time\": end_time, \\\n",
    "                # \"since_id\":since_id, \\  # cannot used with start/end_time!\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fb53b59-a406-4a94-89e8-f62edbad81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_response = connect_to_endpoint(search_url, headers, query_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "780fb3b7-f1d4-41c8-bf4c-6f07cfac8590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'newest_id': '1465470516379021313',\n",
       " 'oldest_id': '1465458465405648903',\n",
       " 'result_count': 100,\n",
       " 'next_token': 'b26v89c19zqg8o3fpdy7o6jo5ebu3ya8iq2kaptu7erul'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#json_response.keys()   # ['data', 'includes', 'meta']\n",
    "json_response['meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3164a63b-3008-49c1-9e55-1a0640f3a055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'tweets', 'media', 'places', 'polls'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response['includes'].keys()  # dict_keys(['users', 'tweets', 'media', 'places', 'polls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "caf50411-a465-4356-b522-f149e8b8023e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['users', 'tweets', 'media', 'places', 'polls'])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response['includes'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "48c9921f-3ea4-4b33-8349-e809fb44d4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reply_settings': 'everyone',\n",
       " 'conversation_id': '1465183027043069959',\n",
       " 'entities': {'mentions': [{'start': 0,\n",
       "    'end': 16,\n",
       "    'username': 'jessica_leigh75',\n",
       "    'id': '2862477860'},\n",
       "   {'start': 17,\n",
       "    'end': 29,\n",
       "    'username': 'EmergencyBK',\n",
       "    'id': '1153682741780598785'}]},\n",
       " 'source': 'Twitter for iPad',\n",
       " 'created_at': '2021-11-29T23:59:32.000Z',\n",
       " 'id': '1465470516379021313',\n",
       " 'public_metrics': {'retweet_count': 0,\n",
       "  'reply_count': 0,\n",
       "  'like_count': 2,\n",
       "  'quote_count': 0},\n",
       " 'author_id': '470651164',\n",
       " 'context_annotations': [{'domain': {'id': '123',\n",
       "    'name': 'Ongoing News Story',\n",
       "    'description': \"Ongoing News Stories like 'Brexit'\"},\n",
       "   'entity': {'id': '1220701888179359745', 'name': 'COVID-19'}}],\n",
       " 'in_reply_to_user_id': '2862477860',\n",
       " 'referenced_tweets': [{'type': 'replied_to', 'id': '1465362278488834049'}],\n",
       " 'possibly_sensitive': False,\n",
       " 'text': '@jessica_leigh75 @EmergencyBK 🤣😂 The new COVID safe medical appointments, Telehealth etc., have been a boon, but can be tricky to negotiate.',\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_response['data'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Google_street_view",
   "language": "python",
   "name": "google_street_view"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
